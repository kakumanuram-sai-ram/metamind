# MetaMind

**Intelligent Dashboard Metadata Extraction and Analysis System for Apache Superset**

MetaMind is a comprehensive system that extracts, analyzes, and consolidates metadata from Apache Superset dashboards using LLM-powered analysis. It identifies tables, columns, relationships, filter conditions, and business context from dashboard SQL queries, then merges metadata from multiple dashboards into a unified knowledge base.

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Installation & Setup](#installation--setup)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Core Components](#core-components)
- [Output Files](#output-files)
- [Technology Stack](#technology-stack)
- [Documentation](#documentation)
- [Contributing](#contributing)

---

## Overview

MetaMind automates the extraction of comprehensive metadata from Superset dashboards, transforming raw dashboard configurations into structured, analyzable metadata. The system:

1. **Extracts** dashboard metadata from Superset API (charts, SQL queries, filters)
2. **Analyzes** SQL queries using LLM to identify tables, columns, and relationships
3. **Enriches** metadata with data types from Trino/Starburst
4. **Merges** metadata from multiple dashboards into unified schemas
5. **Builds** knowledge bases for LLM context injection and documentation

The system supports both single-dashboard extraction and multi-dashboard processing workflows, with parallel processing capabilities and comprehensive progress tracking.

---

## Features

### ğŸ” Intelligent Extraction
- **LLM-Powered Analysis**: Uses Claude Sonnet 4 via DSPy framework to extract tables, columns, and relationships from SQL queries
- **Rule-Based Fallback**: SQL parser for fast extraction when LLM is unavailable
- **Trino Integration**: Automatic column data type detection from Starburst/Trino databases

### ğŸ“Š Comprehensive Metadata
- **Table Metadata**: Descriptions, refresh frequencies, verticals, partition columns
- **Column Metadata**: Data types, descriptions, business context
- **Relationships**: Joining conditions between tables
- **Filter Conditions**: Dashboard-level and chart-level filters
- **Term Definitions**: Business metrics, calculated fields, synonyms

### ğŸ”— Multi-Dashboard Support
- **Parallel Processing**: Extract multiple dashboards simultaneously
- **Intelligent Merging**: LLM-based consolidation with conflict detection and resolution
- **Incremental Updates**: Merge new dashboards with existing consolidated metadata
- **Knowledge Base Generation**: Unified knowledge base from merged metadata

### âš¡ Performance & UX
- **Fast API**: Non-blocking background processing for LLM operations
- **Progress Tracking**: Real-time progress updates for multi-dashboard processing
- **Modern UI**: React-based frontend for dashboard visualization and management
- **RESTful API**: Complete REST API for programmatic access

### ğŸ“ Structured Output
- **Multiple Formats**: JSON, CSV, SQL, TXT files
- **Organized Structure**: Per-dashboard folders with standardized naming
- **Consolidated Metadata**: Merged metadata files for unified schema view
- **Knowledge Base**: Compressed archive ready for LLM context injection

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        React Frontend                           â”‚
â”‚                      (Port 3000/5173)                           â”‚
â”‚              Dashboard Visualization & Management               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ HTTP/REST
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FastAPI Server                             â”‚
â”‚                        (Port 8000)                             â”‚
â”‚  REST API: Extract, List, Download, Progress Tracking          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚          â”‚                    â”‚
    â–¼                 â–¼          â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Superset â”‚    â”‚  Trino   â”‚  â”‚   LLM    â”‚    â”‚  File System â”‚
â”‚   API   â”‚    â”‚   API    â”‚  â”‚ (Claude) â”‚    â”‚   (Output)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Processing Pipeline

```
Dashboard ID(s)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Dashboard Extraction              â”‚
â”‚    - Fetch dashboard metadata        â”‚
â”‚    - Extract charts & SQL queries    â”‚
â”‚    - Export to JSON/CSV/SQL         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Table/Column Extraction            â”‚
â”‚    - LLM-based SQL analysis          â”‚
â”‚    - Rule-based fallback             â”‚
â”‚    - Trino data type enrichment      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Metadata Generation               â”‚
â”‚    - Table metadata                  â”‚
â”‚    - Column metadata                 â”‚
â”‚    - Joining conditions              â”‚
â”‚    - Filter conditions               â”‚
â”‚    - Term definitions                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Multi-Dashboard Merge (Optional)   â”‚
â”‚    - LLM-based consolidation         â”‚
â”‚    - Conflict detection/resolution    â”‚
â”‚    - Unified schema generation       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Knowledge Base Build (Optional)    â”‚
â”‚    - Convert to KB format            â”‚
â”‚    - Create compressed archive       â”‚
â”‚    - Ready for LLM context           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Installation & Setup

### Prerequisites

- Python 3.8+ (Python 3.11 recommended)
- Node.js 16+ (for frontend)
- Access to Superset instance
- Anthropic API key (for LLM features)
- Access to Trino/Starburst (optional, for data type enrichment)

### Backend Setup

1. **Clone/Navigate to the repository:**
   ```bash
   cd /home/devuser/sai_dev/metamind
   ```

2. **Create and activate virtual environment:**
   ```bash
   python -m venv meta_env
   source meta_env/bin/activate  # On Windows: meta_env\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   # Or using uv (if available):
   # ./uv_setup.sh
   ```

4. **Configure authentication:**
   Edit `scripts/config.py`:
   - Set `BASE_URL` to your Superset instance URL
   - Update `HEADERS` with your session cookie and CSRF token
   - See [Authentication Guide](./docs/guides/authentication.md) for details

5. **Set LLM API key:**
   ```bash
   export ANTHROPIC_API_KEY="your-api-key-here"
   # Or set in config.py (not recommended for production)
   ```

### Frontend Setup (Optional)

```bash
cd frontend
npm install
npm start  # Starts on http://localhost:3000
```

### Quick Start Scripts

```bash
# Start backend server
./start_backend.sh

# Start frontend (in separate terminal)
./start_frontend.sh

# Restart API server
./restart_api.sh
```

---

## Usage

### Command Line Interface

#### Single Dashboard Extraction

```bash
# Extract dashboard 729
python scripts/extract_dashboard_with_timing.py 729

# Output files saved to: extracted_meta/729/
```

#### Multi-Dashboard Processing

```bash
# Extract, merge, and build KB for multiple dashboards
python scripts/process_multiple_dashboards.py 585 729 842

# Skip extraction (if already done)
python scripts/process_multiple_dashboards.py 585 729 842 --skip-extract

# Skip merge step
python scripts/process_multiple_dashboards.py 585 729 842 --skip-merge

# Incremental merge (merge with existing consolidated metadata)
python scripts/process_multiple_dashboards.py 585 729 842 --incremental-merge

# Stop on first error
python scripts/process_multiple_dashboards.py 585 729 842 --stop-on-error
```

#### Orchestrator (Extraction Only)

```bash
# Extract multiple dashboards in parallel
python scripts/orchestrator.py 585 729 842

# Sequential processing
python scripts/orchestrator.py 585 729 842 --no-parallel
```

### API Server

#### Start the API Server

```bash
# From scripts directory
python scripts/api_server.py

# Or using uvicorn directly
uvicorn scripts.api_server:app --host 0.0.0.0 --port 8000
```

#### API Endpoints

**Dashboard Management:**
- `GET /api/dashboards` - List all extracted dashboards
- `GET /api/dashboard/{id}/json` - Get dashboard JSON
- `GET /api/dashboard/{id}/csv` - Get dashboard CSV data
- `GET /api/dashboard/{id}/tables-columns` - Get table/column mapping

**Extraction:**
- `POST /api/dashboard/extract` - Extract a single dashboard
- `POST /api/dashboards/process-multiple` - Process multiple dashboards

**Progress Tracking:**
- `GET /api/progress` - Get current processing progress

**Downloads:**
- `GET /api/dashboard/{id}/download/{file_type}` - Download specific file
- `GET /api/dashboard/{id}/download-all` - Download all files as ZIP

See [API Reference](./docs/api-reference.md) for complete documentation.

### Frontend UI

1. Start the backend API server (port 8000)
2. Start the frontend (port 3000)
3. Open http://localhost:3000 in your browser
4. Use the UI to:
   - Extract dashboards by URL or ID
   - View extracted dashboard metadata
   - Download files
   - Process multiple dashboards
   - Monitor progress

---

## Project Structure

```
metamind/
â”œâ”€â”€ scripts/                          # Core Python modules
â”‚   â”œâ”€â”€ api_server.py                 # FastAPI REST API server
â”‚   â”œâ”€â”€ config.py                     # Configuration (URLs, auth, LLM settings)
â”‚   â”œâ”€â”€ query_extract.py              # Superset dashboard extraction
â”‚   â”œâ”€â”€ sql_parser.py                 # Rule-based SQL parsing (fallback)
â”‚   â”œâ”€â”€ llm_extractor.py              # LLM-based extraction using DSPy
â”‚   â”œâ”€â”€ trino_client.py               # Trino/Starburst integration
â”‚   â”œâ”€â”€ metadata_generator.py          # Generate comprehensive metadata files
â”‚   â”œâ”€â”€ orchestrator.py               # Multi-dashboard extraction orchestrator
â”‚   â”œâ”€â”€ merger.py                     # LLM-based metadata merging
â”‚   â”œâ”€â”€ knowledge_base_builder.py     # Knowledge base generation
â”‚   â”œâ”€â”€ progress_tracker.py           # Progress tracking for multi-dashboard
â”‚   â”œâ”€â”€ extract_dashboard_with_timing.py  # Single dashboard extraction entry point
â”‚   â””â”€â”€ process_multiple_dashboards.py    # Multi-dashboard processing entry point
â”‚
â”œâ”€â”€ frontend/                         # React frontend application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js                    # Main application component
â”‚   â”‚   â”œâ”€â”€ components/               # React components
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js                # API client
â”‚   â”‚   â””â”€â”€ styles/                   # Styled components
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ extracted_meta/                   # Output directory
â”‚   â”œâ”€â”€ {dashboard_id}/               # Per-dashboard metadata
â”‚   â”‚   â”œâ”€â”€ {id}_json.json            # Complete dashboard metadata
â”‚   â”‚   â”œâ”€â”€ {id}_csv.csv              # Tabular chart metadata
â”‚   â”‚   â”œâ”€â”€ {id}_queries.sql          # All SQL queries
â”‚   â”‚   â”œâ”€â”€ {id}_tables_columns.csv   # Table-column mapping
â”‚   â”‚   â”œâ”€â”€ {id}_tables_columns_enriched.csv  # With data types
â”‚   â”‚   â”œâ”€â”€ {id}_table_metadata.csv   # Table descriptions
â”‚   â”‚   â”œâ”€â”€ {id}_columns_metadata.csv # Column metadata
â”‚   â”‚   â”œâ”€â”€ {id}_joining_conditions.csv  # Table relationships
â”‚   â”‚   â”œâ”€â”€ {id}_filter_conditions.txt    # Filter conditions
â”‚   â”‚   â””â”€â”€ {id}_definitions.csv          # Term definitions
â”‚   â”‚
â”‚   â”œâ”€â”€ merged_metadata/              # Consolidated metadata (multi-dashboard)
â”‚   â”‚   â”œâ”€â”€ consolidated_table_metadata.csv
â”‚   â”‚   â”œâ”€â”€ consolidated_columns_metadata.csv
â”‚   â”‚   â”œâ”€â”€ consolidated_joining_conditions.csv
â”‚   â”‚   â”œâ”€â”€ consolidated_definitions.csv
â”‚   â”‚   â”œâ”€â”€ consolidated_filter_conditions.txt
â”‚   â”‚   â”œâ”€â”€ conflicts_report.json     # Merge conflicts
â”‚   â”‚   â””â”€â”€ merged_metadata.json      # Complete merged metadata
â”‚   â”‚
â”‚   â”œâ”€â”€ knowledge_base/               # Knowledge base (for LLM/documentation)
â”‚   â”‚   â”œâ”€â”€ table_metadata.json
â”‚   â”‚   â”œâ”€â”€ column_metadata.json
â”‚   â”‚   â”œâ”€â”€ joining_conditions.json
â”‚   â”‚   â”œâ”€â”€ definitions.json
â”‚   â”‚   â”œâ”€â”€ filter_conditions.txt
â”‚   â”‚   â”œâ”€â”€ business_context.json
â”‚   â”‚   â”œâ”€â”€ validations.json
â”‚   â”‚   â””â”€â”€ knowledge_base.zip        # Compressed archive
â”‚   â”‚
â”‚   â””â”€â”€ progress.json                 # Progress tracking state
â”‚
â”œâ”€â”€ docs/                             # Comprehensive documentation
â”‚   â”œâ”€â”€ README.md                     # Documentation index
â”‚   â”œâ”€â”€ system-overview.md            # Architecture overview
â”‚   â”œâ”€â”€ getting-started.md            # Setup guide
â”‚   â”œâ”€â”€ api-reference.md              # API documentation
â”‚   â”œâ”€â”€ adrs/                         # Architecture Decision Records
â”‚   â”œâ”€â”€ modules/                      # Module-specific documentation
â”‚   â””â”€â”€ guides/                       # User guides
â”‚
â”œâ”€â”€ examples/                         # Example usage scripts
â”‚   â”œâ”€â”€ example_usage.py              # Basic extraction examples
â”‚   â””â”€â”€ example_starburst_schema.py   # Schema fetcher examples
â”‚
â”œâ”€â”€ tests/                            # Test files
â”‚   â”œâ”€â”€ test_auth.py                  # Authentication tests
â”‚   â””â”€â”€ test_dashboard_access.py     # Dashboard access tests
â”‚
â”œâ”€â”€ dspy_examples.py                  # DSPy signature examples
â”œâ”€â”€ pyproject.toml                    # Python project configuration
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ start_backend.sh                  # Backend startup script
â”œâ”€â”€ start_frontend.sh                 # Frontend startup script
â””â”€â”€ restart_api.sh                    # API restart script
```

---

## Core Components

### 1. Dashboard Extraction (`query_extract.py`)
- Fetches dashboard metadata from Superset API
- Extracts chart information, SQL queries, and filters
- Exports to JSON, CSV, and SQL formats
- Handles authentication and API errors

### 2. SQL Parser (`sql_parser.py`)
- Rule-based SQL parsing (fallback when LLM unavailable)
- Extracts table and column names from SQL queries
- Identifies original column names (not aliases)
- Normalizes table names (adds catalog prefix if missing)

### 3. LLM Extractor (`llm_extractor.py`)
- Uses DSPy framework with Claude Sonnet 4
- Extracts tables, columns, and relationships from SQL
- Generates comprehensive table and column descriptions
- Extracts joining conditions, filter conditions, and term definitions
- Handles complex SQL with CTEs, subqueries, and aggregations

### 4. Trino Client (`trino_client.py`)
- Connects to Starburst/Trino via Superset SQL Lab API
- Executes DESCRIBE queries to get column data types
- Detects partition columns
- Enriches metadata with database schema information

### 5. Metadata Generator (`metadata_generator.py`)
- Generates comprehensive metadata files:
  - Table metadata with descriptions and context
  - Column metadata with data types and business context
  - Joining conditions between tables
  - Filter conditions (dashboard and chart level)
  - Term definitions (metrics, calculated fields, synonyms)

### 6. Orchestrator (`orchestrator.py`)
- Manages extraction of multiple dashboards
- Supports parallel and sequential processing
- Tracks progress and handles errors
- Organizes output into per-dashboard folders

### 7. Metadata Merger (`merger.py`)
- Merges metadata from multiple dashboards using LLM
- Detects and resolves conflicts (different descriptions, etc.)
- Consolidates into unified schema
- Generates conflict reports
- Supports incremental merging

### 8. Knowledge Base Builder (`knowledge_base_builder.py`)
- Converts merged metadata into knowledge base format
- Creates JSON files optimized for LLM context injection
- Generates compressed archive (knowledge_base.zip)
- Formats data for documentation and business user reference

### 9. Progress Tracker (`progress_tracker.py`)
- Tracks extraction progress for multi-dashboard processing
- Provides real-time status updates via API
- Persists progress to JSON file
- Thread-safe for parallel processing

### 10. API Server (`api_server.py`)
- FastAPI REST API for programmatic access
- Non-blocking background processing for LLM operations
- CORS-enabled for frontend integration
- File download endpoints
- Progress tracking endpoints

---

## Output Files

### Per-Dashboard Files (`extracted_meta/{dashboard_id}/`)

| File | Description |
|------|-------------|
| `{id}_json.json` | Complete dashboard metadata (charts, filters, layout) |
| `{id}_csv.csv` | Tabular representation of chart metadata |
| `{id}_queries.sql` | All SQL queries from dashboard charts |
| `{id}_tables_columns.csv` | Table-to-column mapping (source tables only) |
| `{id}_tables_columns_enriched.csv` | Table-column mapping with data types from Trino |
| `{id}_table_metadata.csv` | Comprehensive table metadata (descriptions, refresh, vertical) |
| `{id}_columns_metadata.csv` | Column metadata (data types, descriptions, business context) |
| `{id}_joining_conditions.csv` | Table relationships and join conditions |
| `{id}_filter_conditions.txt` | Dashboard and chart-level filter conditions |
| `{id}_definitions.csv` | Term definitions (metrics, calculated fields, synonyms) |

### Merged Metadata (`extracted_meta/merged_metadata/`)

| File | Description |
|------|-------------|
| `consolidated_table_metadata.csv` | Unified table metadata from all dashboards |
| `consolidated_columns_metadata.csv` | Unified column metadata from all dashboards |
| `consolidated_joining_conditions.csv` | Unified joining conditions |
| `consolidated_definitions.csv` | Unified term definitions |
| `consolidated_filter_conditions.txt` | All filter conditions |
| `conflicts_report.json` | Report of conflicts detected during merge |
| `merged_metadata.json` | Complete merged metadata in JSON format |

### Knowledge Base (`extracted_meta/knowledge_base/`)

| File | Description |
|------|-------------|
| `table_metadata.json` | Table metadata in LLM-friendly JSON format |
| `column_metadata.json` | Column metadata in LLM-friendly JSON format |
| `joining_conditions.json` | Joining conditions in JSON format |
| `definitions.json` | Term definitions in JSON format |
| `filter_conditions.txt` | Filter conditions (plain text) |
| `business_context.json` | Business context (empty template) |
| `validations.json` | Validation rules (empty template) |
| `knowledge_base.zip` | Compressed archive of all 7 files |

---

## Technology Stack

### Backend
- **Python 3.11** - Core language
- **FastAPI** - REST API framework
- **Uvicorn** - ASGI server
- **Pandas** - Data manipulation and CSV handling
- **DSPy** - LLM framework for structured extraction
- **Requests** - HTTP client for API calls

### LLM
- **Claude Sonnet 4** - Primary LLM (via Anthropic API)
- **Custom API Proxy** - PayTM internal proxy for LLM access

### Frontend
- **React 18** - UI framework
- **Styled Components** - CSS-in-JS styling
- **Axios** - HTTP client
- **React Router** - Client-side routing

### Database Integration
- **Trino/Starburst** - Data warehouse (via Superset SQL Lab API)
- **Superset API** - Dashboard metadata source

### Development Tools
- **UV** - Fast Python package manager (optional)
- **Node.js/npm** - Frontend package management

---

## Documentation

Comprehensive documentation is available in the [`docs/`](./docs/) directory:

### Getting Started
- **[Getting Started Guide](./docs/getting-started.md)** - Setup and first steps
- **[Quick Start](./docs/QUICK_START.md)** - Fast setup guide
- **[Authentication Guide](./docs/guides/authentication.md)** - How to get Superset credentials

### Architecture & Design
- **[System Overview](./docs/system-overview.md)** - High-level architecture
- **[Architecture Decision Records](./docs/adrs/)** - Design decisions and rationale
- **[Execution Flow](./docs/EXECUTION_FLOW.md)** - Processing pipeline details

### API & Usage
- **[API Reference](./docs/api-reference.md)** - Complete API documentation
- **[Module Documentation](./docs/modules/)** - Detailed module docs
- **[Multi-Dashboard Processing](./docs/MULTI_DASHBOARD_PROCESSING.md)** - Multi-dashboard workflow

### Guides
- **[Deployment Guide](./docs/guides/deployment.md)** - Production deployment
- **[Troubleshooting](./docs/guides/troubleshooting.md)** - Common issues and solutions
- **[Download Buttons Guide](./docs/DOWNLOAD_BUTTONS_GUIDE.md)** - File download features

### Reference
- **[File Organization](./docs/FILES_ORGANIZATION.md)** - Output file structure
- **[Key Files Guide](./docs/KEY_FILES_GUIDE.md)** - Important files overview
- **[DSPy Examples](./docs/DSPY_EXAMPLES_GUIDE.md)** - DSPy signature examples

---

## Contributing

### Development Workflow

1. **Create a virtual environment:**
   ```bash
   python -m venv meta_env
   source meta_env/bin/activate
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Run tests:**
   ```bash
   python -m pytest tests/
   ```

4. **Follow code style:**
   - Use type hints where possible
   - Add docstrings to functions and classes
   - Follow existing code patterns

### Key Design Principles

- **Modularity**: Each component has a single responsibility
- **LLM-First**: Prefer LLM extraction over rule-based when possible
- **Fallback Support**: Always provide rule-based fallback
- **Thread Safety**: Multi-dashboard processing must be thread-safe
- **Progress Tracking**: Long-running operations should report progress
- **Error Handling**: Continue processing on errors when possible

### Architecture Decision Records

See [`docs/adrs/`](./docs/adrs/) for documented design decisions:
- ADR-001: LLM-based extraction
- ADR-002: DSPy framework choice
- ADR-003: File organization
- ADR-004: Background processing
- ADR-005: Thread safety
- ADR-006: Trino integration

---

## License

Internal use only.

---

## Support

For issues, questions, or contributions:
1. Check the [Troubleshooting Guide](./docs/guides/troubleshooting.md)
2. Review relevant documentation in [`docs/`](./docs/)
3. Check existing issues or create a new one

---

**MetaMind** - Intelligent Dashboard Metadata Extraction System

