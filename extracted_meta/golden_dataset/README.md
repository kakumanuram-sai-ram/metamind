# Golden Dataset

This directory contains golden datasets of natural language questions paired with SQL queries from dashboard metadata.

## Overview

The golden dataset is generated using LLM (Claude Sonnet 4) to convert SQL queries from Superset dashboards into natural language questions that users might ask. This dataset is useful for:

- **Training text-to-SQL models**: Pairs of natural language questions with corresponding SQL queries
- **Benchmarking**: Testing the accuracy of SQL generation systems
- **Fine-tuning**: Creating domain-specific training data for your organization's data

## Dataset Format

Each CSV file contains the following columns:

| Column | Description |
|--------|-------------|
| `dashboard_id` | The Superset dashboard ID |
| `chart_id` | The chart ID within the dashboard |
| `chart_name` | The name/title of the chart |
| `user_query` | Natural language question generated by LLM that represents what a user might ask |
| `sql_query` | The actual SQL query that generates the chart data |

## File Naming Convention

- **Single dashboard**: `golden_dataset_{dashboard_id}.csv`
  - Example: `golden_dataset_964.csv`
- **Multiple dashboards**: `golden_dataset_combined_{dashboard_ids}.csv`
  - Example: `golden_dataset_combined_964_476_729.csv`

## Generation Process

The golden dataset is generated using the `golden_dataset_generator.py` script:

```bash
# Generate for a single dashboard
python3 scripts/golden_dataset_generator.py 964

# Generate for multiple dashboards
python3 scripts/golden_dataset_generator.py 964 476 729

# Specify custom output path
python3 scripts/golden_dataset_generator.py 964 --output my_dataset.csv
```

### How It Works

1. **Extract**: Reads dashboard JSON metadata from `extracted_meta/{dashboard_id}/`
2. **Process**: For each chart in the dashboard:
   - Extracts the SQL query
   - Uses Claude Sonnet 4 (via DSPy) to generate a natural language question
   - The LLM is prompted to create business-focused questions, not technical ones
3. **Save**: Outputs a CSV file with all question-SQL pairs

### LLM Prompt Strategy

The LLM is given:
- **Chart name**: Context about what the chart displays
- **Dashboard title**: Overall dashboard context
- **SQL query**: The actual query to convert

And asked to generate:
- A clear, concise business question
- Focused on what the user wants to know, not how to get it
- No technical details like table names or SQL syntax

## Viewing the Dataset

Use the viewer script for a readable display:

```bash
# View all entries
python3 scripts/view_golden_dataset.py extracted_meta/golden_dataset/golden_dataset_964.csv

# View first 3 entries
python3 scripts/view_golden_dataset.py extracted_meta/golden_dataset/golden_dataset_964.csv --rows 3
```

## Example Entries

### Entry 1: Monthly Performance Comparison
- **Dashboard**: UPI Traffic Dashboard (964)
- **Chart**: UPI Traffic Dashboard
- **User Query**: "How are our UPI traffic campaigns performing this month compared to last month in terms of impressions, clicks, and click-through rates across different categories?"
- **SQL**: Complex aggregation with MTD/LMTD filtering

### Entry 2: Daily Trend Analysis
- **Dashboard**: UPI Traffic Dashboard (964)
- **Chart**: DOD Impressions
- **User Query**: "What is the daily click-through rate trend for banner campaigns on the UPI home page over the past two months?"
- **SQL**: Time-series query with date truncation

## Quality Considerations

The quality of generated questions depends on:

1. **SQL Query Quality**: Well-structured SQL with clear metrics produces better questions
2. **Chart Names**: Descriptive chart names help the LLM understand context
3. **Dashboard Context**: Dashboard title provides business domain context

## Integration with MetaMind

This golden dataset is part of the MetaMind metadata extraction system:

```
MetaMind/
├── scripts/
│   ├── golden_dataset_generator.py  # Generator script
│   └── view_golden_dataset.py       # Viewer script
└── extracted_meta/
    ├── golden_dataset/              # This directory
    │   ├── README.md                # This file
    │   └── golden_dataset_*.csv     # Generated datasets
    └── {dashboard_id}/              # Source metadata
        └── {dashboard_id}_json.json # Source data
```

## API Configuration

The generator uses the same LLM configuration as the main MetaMind system:

- **Model**: Claude Sonnet 4 (`anthropic/claude-sonnet-4`)
- **API**: Internal proxy (`https://cst-ai-proxy.paytm.com`)
- **Framework**: DSPy for LLM orchestration

Configuration is loaded from `scripts/config.py`.

## Statistics (Dashboard 964)

- **Dashboard ID**: 964
- **Dashboard Title**: UPI Traffic Dashboard
- **Total Charts**: 10
- **Question-SQL Pairs**: 10
- **Average Question Length**: ~100 characters
- **Domain**: UPI traffic metrics and campaign performance

## Future Enhancements

Potential improvements:

1. **Multi-turn conversations**: Generate follow-up questions
2. **Question variations**: Generate multiple phrasings for the same SQL
3. **Difficulty levels**: Tag questions as easy/medium/hard
4. **Validation**: Human review and rating of question quality
5. **Augmentation**: Add negative examples (invalid questions)

## Usage Examples

### Training Text-to-SQL Models

```python
import pandas as pd

# Load dataset
df = pd.read_csv('extracted_meta/golden_dataset/golden_dataset_964.csv')

# Prepare training data
train_data = df[['user_query', 'sql_query']].values.tolist()

# Use for fine-tuning or few-shot learning
```

### Benchmarking SQL Generation

```python
# Test your SQL generator
for _, row in df.iterrows():
    question = row['user_query']
    expected_sql = row['sql_query']
    
    generated_sql = your_sql_generator(question)
    
    # Compare generated vs expected
    accuracy = compute_accuracy(generated_sql, expected_sql)
```

## Contact

For questions or issues related to the golden dataset generation, refer to the main MetaMind documentation or the development team.



